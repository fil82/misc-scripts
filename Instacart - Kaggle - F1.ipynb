{"cells":[{"metadata":{"_uuid":"77c844eccae1919e94ed779f429b5ec05988fbec","_cell_guid":"2661ec78-7389-4fbc-89b4-9c498a996dbe"},"cell_type":"markdown","source":"This LightGBM runs on Kaggle as well as on my 3-year old i5/8GB Surface laptop in less than 8 minutes to generate a submission. It has 19 features and produced LB 0.3815 (24% when first submitted). Part of it is similar to the many public kernels (such as Fabien Vavrand, Khaled Elshamouty, Paul-Antoine Nguyen, China, etc - no idea who was the first) except a bit faster and some of the features and parameters are different. For example, I noticed that some people chose dtypes carefully when reading the raw data but then use LighGBM with unnecessarily large dtypes (my X_train is about 173MB with all the data). What is new is the addition of the F1 optimization vs cart size which is reproduced in a shortened version (to finish in 15 minutes) and which has been my concern from the begining (when I started to look at the problem without any ML algorithm and used simply the most reordered items to get 0.32 for the correct basket size and 0.34 for 40% larger cart). ML did not change that. "},{"metadata":{"_uuid":"444c60d5d818b00f785962efbf86d58fa180e17b","_cell_guid":"f17a99d0-84fe-4d92-862f-80d4bdf792cf"},"cell_type":"markdown","source":"![alt text](F1_vs_mean_cart_size.jpg \"F1 vs Mean Cart Size\")"},{"metadata":{"_uuid":"c62bc8070d7acfcc4befca1a993556863cce0275","_cell_guid":"3a901a44-50f0-4579-ac84-d27c809d1f94","trusted":false},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport gc\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\nmyfolder = '../input/'\nprint('loading files ...')\n\nprior = pd.read_csv(myfolder + 'order_products__prior.csv', dtype={'order_id': np.uint32,\n           'product_id': np.uint16, 'reordered': np.uint8, 'add_to_cart_order': np.uint8})\n\ntrain_orders = pd.read_csv(myfolder + 'order_products__train.csv', dtype={'order_id': np.uint32,\n           'product_id': np.uint16, 'reordered': np.int8, 'add_to_cart_order': np.uint8 })\n\norders = pd.read_csv(myfolder + 'orders.csv', dtype={'order_hour_of_day': np.uint8,\n           'order_number': np.uint8, 'order_id': np.uint32, 'user_id': np.uint32,\n           'order_dow': np.uint8, 'days_since_prior_order': np.float16})\n\norders.eval_set = orders.eval_set.replace({'prior': 0, 'train': 1, 'test':2}).astype(np.uint8)\norders.days_since_prior_order = orders.days_since_prior_order.fillna(30).astype(np.uint8)\n\nproducts = pd.read_csv(myfolder + 'products.csv', dtype={'product_id': np.uint16,\n            'aisle_id': np.uint8, 'department_id': np.uint8},\n             usecols=['product_id', 'aisle_id', 'department_id'])\n\nprint('done loading')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b5e6da0efd00399b5f57ce608a507f6e5062103d","_cell_guid":"50a4a0e4-fa87-46a1-87d2-f7d354775387","trusted":false},"cell_type":"code","source":"print('merge prior and orders and keep train separate ...')\n\norders_products = orders.merge(prior, how = 'inner', on = 'order_id')\ntrain_orders = train_orders.merge(orders[['user_id','order_id']], left_on = 'order_id', right_on = 'order_id', how = 'inner')\n\ndel prior\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"41f7fd7cdd0c726597d81b122561c54d058e52be","_cell_guid":"f8df7c23-333d-4e24-9fe5-e387cd9207c5","trusted":false},"cell_type":"code","source":"print('Creating features I ...')\n\n# sort orders and products to get the rank or the reorder frequency\nprdss = orders_products.sort_values(['user_id', 'order_number', 'product_id'], ascending=True)\nprdss['product_time'] = prdss.groupby(['user_id', 'product_id']).cumcount()+1\n\n# getting products ordered first and second times to calculate probability later\nsub1 = prdss[prdss['product_time'] == 1].groupby('product_id').size().to_frame('prod_first_orders')\nsub2 = prdss[prdss['product_time'] == 2].groupby('product_id').size().to_frame('prod_second_orders')\nsub1['prod_orders'] = prdss.groupby('product_id')['product_id'].size()\nsub1['prod_reorders'] = prdss.groupby('product_id')['reordered'].sum()\nsub2 = sub2.reset_index().merge(sub1.reset_index())\nsub2['prod_reorder_probability'] = sub2['prod_second_orders']/sub2['prod_first_orders']\nsub2['prod_reorder_ratio'] = sub2['prod_reorders']/sub2['prod_orders']\nprd = sub2[['product_id', 'prod_orders','prod_reorder_probability', 'prod_reorder_ratio']]\n\ndel sub1, sub2, prdss\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dfe1b4471fab2dfee4241f3efbbbfda2169c4ff7","_cell_guid":"28d00402-9eb8-481c-a147-274d55d4e723","trusted":false},"cell_type":"code","source":"print('Creating features II ...')\n\n# extracting prior information (features) by user\nusers = orders[orders['eval_set'] == 0].groupby(['user_id'])['order_number'].max().to_frame('user_orders')\nusers['user_period'] = orders[orders['eval_set'] == 0].groupby(['user_id'])['days_since_prior_order'].sum()\nusers['user_mean_days_since_prior'] = orders[orders['eval_set'] == 0].groupby(['user_id'])['days_since_prior_order'].mean()\n\n# merging features about users and orders into one dataset\nus = orders_products.groupby('user_id').size().to_frame('user_total_products')\nus['eq_1'] = orders_products[orders_products['reordered'] == 1].groupby('user_id')['product_id'].size()\nus['gt_1'] = orders_products[orders_products['order_number'] > 1].groupby('user_id')['product_id'].size()\nus['user_reorder_ratio'] = us['eq_1'] / us['gt_1']\nus.drop(['eq_1', 'gt_1'], axis = 1, inplace = True)\nus['user_distinct_products'] = orders_products.groupby(['user_id'])['product_id'].nunique()\n\n# the average basket size of the user\nusers = users.reset_index().merge(us.reset_index())\nusers['user_average_basket'] = users['user_total_products'] / users['user_orders']\n\nus = orders[orders['eval_set'] != 0]\nus = us[['user_id', 'order_id', 'eval_set', 'days_since_prior_order']]\nusers = users.merge(us)\n\ndel us\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"34bbe83997c1f404764ba060fabcf005d208e8ba","_cell_guid":"ddd52e84-2d8f-4b76-b50c-b19cc7adeb98","trusted":false},"cell_type":"code","source":"print('Finalizing features and the main data file  ...')\n# merging orders and products and grouping by user and product and calculating features for the user/product combination\ndata = orders_products.groupby(['user_id', 'product_id']).size().to_frame('up_orders')\ndata['up_first_order'] = orders_products.groupby(['user_id', 'product_id'])['order_number'].min()\ndata['up_last_order'] = orders_products.groupby(['user_id', 'product_id'])['order_number'].max()\ndata['up_average_cart_position'] = orders_products.groupby(['user_id', 'product_id'])['add_to_cart_order'].mean()\ndata = data.reset_index()\n\n#merging previous data with users\ndata = data.merge(prd, on = 'product_id')\ndata = data.merge(users, on = 'user_id')\n\n#user/product combination features about the particular order\ndata['up_order_rate'] = data['up_orders'] / data['user_orders']\ndata['up_orders_since_last_order'] = data['user_orders'] - data['up_last_order']\ndata = data.merge(train_orders[['user_id', 'product_id', 'reordered']], \n                  how = 'left', on = ['user_id', 'product_id'])\ndata = data.merge(products, on = 'product_id')\n\ndel orders_products     #, orders, train_orders\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"66e5a8bb1dfce21f189f1245cf1d78d2a26c4b30","_cell_guid":"0c4c8182-145a-4e03-9d0d-6607ca951448","trusted":false},"cell_type":"code","source":"print(' Training data for later use in F1 vs cart size only  ...')\n\n#save the actual reordered products of the train set in a list format and then delete the original frames\ntrain_orders = train_orders[train_orders['reordered']==1].drop('reordered',axis=1)\norders.set_index('order_id', drop=False, inplace=True)\ntrain1=orders[['order_id','user_id']].loc[orders['eval_set']==1]\ntrain1['actual'] = train_orders.groupby('order_id').aggregate({'product_id':lambda x: list(x)})\ntrain1['actual']=train1['actual'].fillna('')\nn_actual = train1['actual'].apply(lambda x: len(x)).mean()   # this is the average cart size\n\ndel orders, train_orders\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"163284d6d9f8f6d89c2d0fffb2a624143c25bc18","_cell_guid":"416bce6e-6aa2-4ae8-852c-26169c1e97d1","trusted":false},"cell_type":"code","source":"print('setting dtypes for data ...')\n\n#reduce the size by setting data types\ndata = data.astype(dtype= {'user_id' : np.uint32, 'product_id'  : np.uint16,\n            'up_orders'  : np.uint8, 'up_first_order' : np.uint8, 'up_last_order' : np.uint8,\n            'up_average_cart_position' : np.uint8, 'prod_orders' : np.uint16, \n            'prod_reorder_probability' : np.float16,   \n            'prod_reorder_ratio' : np.float16, 'user_orders' : np.uint8,\n            'user_period' : np.uint8, 'user_mean_days_since_prior' : np.uint8,\n            'user_total_products' : np.uint8, 'user_reorder_ratio' : np.float16, \n            'user_distinct_products' : np.uint8, 'user_average_basket' : np.uint8,\n            'order_id'  : np.uint32, 'eval_set' : np.uint8, \n            'days_since_prior_order' : np.uint8, 'up_order_rate' : np.float16, \n            'up_orders_since_last_order':np.uint8,\n            'aisle_id': np.uint8, 'department_id': np.uint8})\n\ndata['reordered'].fillna(0, inplace=True)  # replace NaN with zeros (not reordered) \ndata['reordered']=data['reordered'].astype(np.uint8)\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4e5b51f7927640182c87a77f48c901f6eb48f2fc","_cell_guid":"b91a1921-6197-4b3b-9bc5-5644cf0a2b8a","trusted":false},"cell_type":"code","source":"print('Preparing Train and Test sets ...')\n\n# filter by eval_set (train=1, test=2) and dropp the id's columns (not part of training features) \n# but keep prod_id and user_id in test\n\ntrain = data[data['eval_set'] == 1].drop(['eval_set', 'user_id', 'product_id', 'order_id'], axis = 1)\ntest =  data[data['eval_set'] == 2].drop(['eval_set', 'user_id', 'reordered'], axis = 1)\n\n#del data\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dd311041a55557dd81c1e271ce9c0283cc6b2e66","_cell_guid":"965ad5b2-f372-4687-adc6-3f197d043a4b","trusted":false},"cell_type":"code","source":"print('preparing X,y for LightGBM ...')\n\n# for preliminary runs sample a fraction of the data by (un)commenting the next two lines\n#print('sampling train data ...')\n#train = train.sample(frac=0.25)\n\n# Splitting the training set to train and validation set\nX_train, X_eval, y_train, y_eval = train_test_split(\n    train[train.columns.difference(['reordered'])], train['reordered'], test_size=0.33)\n\ndel train\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"930040209e8d6bae5d49c19c90377dde2c4a7c0b","_cell_guid":"558b12af-32db-4557-81f5-888ec7349e01","trusted":false},"cell_type":"code","source":"print('formatting and training LightGBM ...')\n\nlgb_train = lgb.Dataset(X_train, label=y_train)\nlgb_eval = lgb.Dataset(X_eval, y_eval, reference = lgb_train)\n\n# there is some room to change the parameters and improve - I have not done it systematically\n# for example change num_boost_round to 200\n\nparams = {'task': 'train', 'boosting_type': 'gbdt',   'objective': 'binary', 'metric': {'binary_logloss', 'auc'},\n    'num_iterations' : 1000, 'max_bin' : 100, 'num_leaves': 512, 'feature_fraction': 0.6, 'learning_rate' : 0.05}\n\nlgb_model = lgb.train(params, lgb_train, num_boost_round = 75, valid_sets = lgb_eval, early_stopping_rounds=10)\n\ndel lgb_train, X_train, y_train\ngc.collect()\n\nprint('applying model to test data ...')\ntest['reordered'] = lgb_model.predict(test[test.columns.difference(\n    ['order_id', 'product_id'])], num_iteration = lgb_model.best_iteration)\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c25b17c043b87d87035a91b4169fdb2b4e96cb65","_cell_guid":"5d21d418-cc56-4f20-a806-18e7be1b5cc5","collapsed":true,"trusted":false},"cell_type":"code","source":"print('formatting and writing to submission file ...')\n\n#set the threshold z (should be optimized for F1) \nz=0.22\nprd_bag = dict()\nfor row in test.itertuples():\n    if row.reordered > z:   \n        try:\n            prd_bag[row.order_id] += ' ' + str(row.product_id)\n        except:\n            prd_bag[row.order_id] = str(row.product_id)\n\nfor order in test.order_id:\n    if order not in prd_bag:\n        prd_bag[order] = 'None'\n\nsubmit = pd.DataFrame.from_dict(prd_bag, orient='index')\nsubmit.reset_index(inplace=True)\nsubmit.columns = ['order_id', 'products']\nsubmit.to_csv('LightGBM_submit22.csv', index=False)\nprint(submit['products'].apply(lambda x: len(x.split())).mean())\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8609023a59305ddb033ba4d3f2c701778813be97","_cell_guid":"82b91729-b9ec-4ca1-ab00-15dc4f8afb1c","collapsed":true,"trusted":false},"cell_type":"code","source":"# check feature importance\nlgb.plot_importance(lgb_model, figsize=(7,9))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0666575ca104a3a8feb701706173a85353bddd63","_cell_guid":"0b462095-1ccc-4313-99a1-0cb41c20b544","collapsed":true,"trusted":false},"cell_type":"code","source":"print(' F1 vs cart size analysis ...')\n\ncheck =  data[data['eval_set'] == 1].drop(['eval_set', 'user_id', 'reordered'], axis = 1)\n\ncheck['reordered'] = lgb_model.predict(check[check.columns.difference(\n    ['order_id', 'product_id'])], num_iteration = lgb_model.best_iteration)\n\nprint(' running ...')\n\ndef f1_score_single(x):                 #from LiLi\n    y_true = set(x.actual)\n    y_pred = set(x.list_prod)\n    cross_size = len(y_true & y_pred)\n    if cross_size == 0: return 0.\n    p = 1. * cross_size / len(y_pred)\n    r = 1. * cross_size / len(y_true)\n    return 2 * p * r / (p + r)\n\nF1array=np.array([])\nnarray=np.array([])\nzarray=np.array([])\nactualarray=np.array([])\n\n# change the range to 0.12 - 0.30 in order to produce data for the chart in the introduction\n# otherwise there may not be enough time for it to run on Kaggle\n#for z in np.arange(0.12,0.31,0.01):\nfor z in np.arange(0.22,0.23,0.01):    \n\n    prd_bag = dict()\n    for row in check.itertuples():\n        if row.reordered > z:   \n            try:\n                prd_bag[row.order_id] += ' ' + str(row.product_id)\n            except:\n                prd_bag[row.order_id] = str(row.product_id)\n\n    for order in check.order_id:\n        if order not in prd_bag:\n            prd_bag[order] = ' '\n\n    submit2 = pd.DataFrame.from_dict(prd_bag, orient='index')\n    submit2.reset_index(inplace=True)\n    submit2.columns = ['order_id', 'products']\n    submit2['list_prod']=submit2['products'].apply(lambda x: list(map(int, x.split())))\n    n = submit2['products'].apply(lambda x: len(x.split())).mean()\n        \n    predact=pd.merge(train1,submit2,on='order_id',how='inner')\n    predact['f1']=predact.apply(f1_score_single,axis=1)\n    F1 = predact['f1'].mean()\n    \n    F1array=np.append(F1array,F1)\n    narray=np.append(narray,n)\n    zarray=np.append(zarray,z)\n    actualarray=np.append(actualarray,n_actual)\n    \n    print(' F1, n, z, n_actual :  ', F1,n,z,n_actual)\n    \nprint(' done ')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"af8ed0e28d032275a62ded94011fb4fc20e40d7b","_cell_guid":"d4cc37ab-9c43-483c-beb1-b6bbf8d9599d","trusted":true},"cell_type":"code","source":"Y1 =[0.368,0.373,0.377,0.3801,0.382,0.3828,0.3832,0.3825,0.3815,0.3796,\n         0.3771,0.3744,0.3713,0.3677,0.3636,0.3591,0.3542,0.349,0.3438]\nX=np.arange(0.12,0.31,0.01)\nY2 = np.empty(19)\nY2.fill(6.31)\nY3=[15.5,14.3,13.3,12.3,11.5,10.8,10.1,9.53,8.97,8.46,7.99,7.55,7.15,6.77,6.41,6.08,5.78,5.4,5.2]\n#replace X,Y1,Y2,Y3 with zarray,F1array,actualarray,narray to update\n\nplt.clf()\nfig = plt.figure()\nax = fig.add_subplot(111)\nlns1 = ax.plot(X, Y2, '-', label = 'Actual')\nlns2 = ax.plot(X, Y3, '-', label = 'Predicted')\nax2 = ax.twinx()\nlns3 = ax2.plot(X, Y1, '-r', label = 'F1')\nlns = lns1+lns2+lns3\nlabs = [l.get_label() for l in lns]\nax.legend(lns, labs, loc=0)\nax.set_xlabel('Threshold')\nax.set_ylabel('Mean Cart Size')\nax2.set_ylabel('F1')\nplt.suptitle('F1 vs Mean Cart Size', size=12)\nplt.savefig('F1_vs_mean_cart_size.jpg')\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"anaconda-cloud":{},"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"},"language_info":{"pygments_lexer":"ipython3","mimetype":"text/x-python","nbconvert_exporter":"python","version":"3.6.1","codemirror_mode":{"version":3,"name":"ipython"},"file_extension":".py","name":"python"}},"nbformat":4,"nbformat_minor":1}